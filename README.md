This repository provides the reproduction of the paper “Contrastive Change Captioning with Cross-modal Attention” (AAAI 2025).

I was impressed by the remarkable results reported in the SSGPA paper, which demonstrate significant improvements over previous methods. However, after carefully reviewing the paper, I found several aspects—such as architectural design, methodological descriptions, and experimental configurations—either unclear or potentially inconsistent. These concerns were also echoed in discussions with other researchers working in related areas.

I contacted the authors via email twice to request access to the code and pretrained checkpoints. Unfortunately, due to various constraints, they were unable to share them at this time.

Below, I summarize the main challenges I encountered during the review and reproduction process:
	•	Inconsistencies in the Model Architecture: The paper describes conflicting visual pipelines. The architecture figure suggests a frozen ViT with a Fusion Change Adapter Encoder (similar to VIR-VLFM), while the main text also refers to a ResNet backbone with a global-local attention module. Moreover, the experiment section mentions both ResNet-101 and EVA-ViT-g/14 as backbones, making it unclear which setup was actually used.
	•	GPU Usage and Training Requirements: The paper claims that all experiments were conducted on a single RTX 3090 (24GB). However, our attempts to replicate the setup—using either ResNet or EVA-ViT-g/14 with Vicuna-7B—consistently required 30–70 GB of GPU memory, even with mixed precision and memory optimizations. This raises concerns about the feasibility of the reported hardware configuration.
	•	Ambiguous Vision-Language Integration: The model is described as directly feeding image features into a frozen Vicuna-7B, without any bridging modules such as a Q-Former, projection layers, or fine-tuning. It remains unclear how cross-modal alignment is achieved, especially given the significant performance gains reported.
	•	Difficulty Interpreting the Consistency Constraint: The consistency mechanism described in the paper is not formally defined, and its practical implementation remains vague, which complicates reproduction.
	•	Unrealistic Visualization Quality: The visualizations of change localization in the paper show highly accurate and regular contours that differ from typical results in this task. This suggests the possible use of post-processing or additional heuristics, which were not disclosed.
	•	Missing Implementation Details: Several key settings—such as input resolution, loss weight configuration, and training schedule—are missing or only briefly mentioned.

In light of these issues, I attempted to reproduce the results based on the paper’s descriptions and my own understanding, using commonly adopted practices in the field. I welcome any corrections or feedback from the authors. The following sections provide a detailed analysis of these issues and present the results from my reproduction attempt.